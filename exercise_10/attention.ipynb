{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11 Part 1: Self-Attention\n",
    "**Summer Semester 2024**\n",
    "\n",
    "**Author**: Stefan Baumann (stefan.baumann@lmu.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Implement Self-Attention\n",
    "In this exercise, you will implement multi-head self-attention for a 2D sequence of tokens (shape `B D H W`) yourself using **only basic functions (no pre-made attention implementations!)**. You're allowed to use simple functions such as, e.g., `torch.bmm()`, `torch.nn.functional.softmax()`, ... and simple modules such as `torch.nn.Linear`.\n",
    "\n",
    "Usage of functions provided by the `einops` library (such as `einops.rearrange()`) is also allowed and encouraged (but completely optional!), as it allows writing the code in a nice and concise way by specifying operations across axes of tensors as strings instead of relying on dimension indices.<br>\n",
    "A short introduction into einops is available at https://nbviewer.org/github/arogozhnikov/einops/blob/master/docs/1-einops-basics.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device \"cuda\".\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optional\n",
    "import einops\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device \"{device}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 256,\n",
    "        head_dim: int = 32,\n",
    "        value_dim: int = 32,\n",
    "        num_heads: int = 8,\n",
    "    ):\n",
    "        \"\"\"Multi-Head Self-Attention Module with 2d token input & output\n",
    "\n",
    "        Args:\n",
    "            embed_dim (int, optional): Dimension of the tokens at the input & output. Defaults to 256.\n",
    "            head_dim (int, optional): Per-head dimension of query & key. Defaults to 32.\n",
    "            value_dim (int, optional): Per-head dimension of values. Defaults to 32.\n",
    "            num_heads (int, optional): Number of attention heads. Defaults to 6.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.value_dim = value_dim\n",
    "\n",
    "        self.q = nn.Linear(embed_dim, num_heads * head_dim, bias=False)\n",
    "        self.k = nn.Linear(embed_dim, num_heads * head_dim, bias=False)\n",
    "        self.v = nn.Linear(embed_dim, num_heads * value_dim, bias=False)\n",
    "        self.out = nn.Linear(num_heads * value_dim, embed_dim, bias=False)\n",
    "        self.scale = 1 / math.sqrt(self.head_dim)\n",
    "\n",
    "        # Hint: use a single linear layer for q/k/v/out each, and name the respective layers q, k, v, out for the unit tests below to work.\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward of multi-head self-attention\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, D, H, W) (batch, embedding dimension, height, width)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (B, D, H, W) (batch, embedding dimension, height, width)\n",
    "        \"\"\"\n",
    "        B, D, H, W = x.shape\n",
    "\n",
    "      # Reshape and transpose input to (B, H*W, D)\n",
    "        x = rearrange(x, 'b d h w -> b (h w) d')\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.q(x)  # (B, H*W, num_heads * head_dim)\n",
    "        K = self.k(x)  # (B, H*W, num_heads * head_dim)\n",
    "        V = self.v(x)  # (B, H*W, num_heads * value_dim)\n",
    "\n",
    "        # Split into multiple heads\n",
    "        Q = rearrange(Q, 'b n (h d) -> b h n d', h=self.num_heads)\n",
    "        K = rearrange(K, 'b n (h d) -> b h n d', h=self.num_heads)\n",
    "        V = rearrange(V, 'b n (h d) -> b h n d', h=self.num_heads)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Compute weighted sum of values\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # Reshape back to (B, H*W, num_heads * value_dim)\n",
    "        attention_output = rearrange(attention_output, 'b h n d -> b n (h d)')\n",
    "\n",
    "        # Final linear transformation\n",
    "        output = self.out(attention_output)  # (B, H*W, embed_dim)\n",
    "\n",
    "        # Reshape back to (B, D, H, W)\n",
    "        output = rearrange(output, 'b (h w) d -> b d h w', h=H, w=W)\n",
    "        return output\n",
    "\n",
    "\n",
    "        # TODO: Student. Don't forget to implement scaling of the attention logits by 1/sqrt(head_dim).\n",
    "        # Implement a standard multi-head self-attention mechanism in a fully batched manner (no explicit for loops etc, pure PyTorch/einops code)\n",
    "        # The expected behavior of this method is that described in Eq. 2 of Attention Is All You Need, Vaswani et al., 2017, NeurIPS.\n",
    "        # In the case of single-head attention, the expected behavior is described by Eq. 1 of the same paper\n",
    "        # Hint when you run into problems:\n",
    "        # For consistency with the multi-head reference implementation the unit test compares against, make sure that the individual heads are arranged correctly in q, k, v, and out.\n",
    "        # The convention is that each head's part in q/k/v is contiguous, i.e., if you want to get the query for head 0, it's at q[..., :head_dim], head 1 is at q[..., head_dim:2*head_dim], etc.\n",
    "\n",
    "# Unit Test (single-head) DO NOT CHANGE!\n",
    "with torch.no_grad():\n",
    "    layer = SelfAttention2d(embed_dim=256, head_dim=256, value_dim=256, num_heads=1).to(device)\n",
    "    x = torch.randn((4, 256, 24, 24), device=device)\n",
    "    res_layer = layer(x)\n",
    "\n",
    "    layer_ref = nn.MultiheadAttention(layer.embed_dim, layer.num_heads).to(device)\n",
    "    layer_ref.load_state_dict({ 'in_proj_weight': torch.cat([layer.q.weight, layer.k.weight, layer.v.weight]), 'out_proj.weight': layer.out.weight }, strict=False)\n",
    "    res_ref = layer_ref(*[x.view(*x.shape[:2], -1).permute(2, 0, 1)] * 3)[0].permute(1, 2, 0).view(*x.shape)\n",
    "    assert torch.allclose(res_layer, res_ref, rtol=1e-2, atol=1e-5), 'Single-head attention result incorrect.'\n",
    "\n",
    "# Unit Test (multi-head) DO NOT CHANGE!\n",
    "with torch.no_grad():\n",
    "    layer = SelfAttention2d().to(device)\n",
    "    x = torch.randn((4, 256, 24, 24), device=device)\n",
    "    res_layer = layer(x)\n",
    "\n",
    "    layer_ref = nn.MultiheadAttention(layer.embed_dim, layer.num_heads).to(device)\n",
    "    layer_ref.load_state_dict({ 'in_proj_weight': torch.cat([layer.q.weight, layer.k.weight, layer.v.weight]), 'out_proj.weight': layer.out.weight }, strict=False)\n",
    "    res_ref = layer_ref(*[x.view(*x.shape[:2], -1).permute(2, 0, 1)] * 3)[0].permute(1, 2, 0).view(*x.shape)\n",
    "    assert torch.allclose(res_layer, res_ref, rtol=1e-2, atol=1e-5), 'Multi-head attention result incorrect.'\n",
    "\n",
    "print('All tests passed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
